@article{agrawalMetalearningBasedGenerative2023,
  title = {A {{Meta-learning Based Generative Model}} with {{Graph Attention Network}} for {{Multi-Modal Recommender Systems}}},
  author = {Agrawal, Pawan and Raj, Subham and Saha, Sriparna and Onoe, Naoyuki},
  date = {2023},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {222},
  pages = {581--590},
  issn = {18770509},
  doi = {10.1016/j.procs.2023.08.196},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050923009614},
  urldate = {2024-09-11},
  langid = {english},
  file = {/Users/alexander/Zotero/storage/GMUYT6ZE/Agrawal et al. - 2023 - A Meta-learning Based Generative Model with Graph Attention Network for Multi-Modal Recommender Syst.pdf}
}

@article{alcala-fdezKEELDataMiningSoftware2011,
  title = {{{KEEL Data-Mining Software Tool}}: {{Data Set Repository}}, {{Integration}} of {{Algorithms}} and {{Experimental Analysis Framework}}},
  shorttitle = {{{KEEL Data-Mining Software Tool}}},
  author = {Alcalá-Fdez, J. and Fernández, Alberto and Luengo, Julián and Derrac, J. and García, S.},
  date = {2011},
  journaltitle = {J. Multiple Valued Log. Soft Comput.},
  url = {https://www.semanticscholar.org/paper/KEEL-Data-Mining-Software-Tool%3A-Data-Set-of-and-Alcal%C3%A1-Fdez-Fern%C3%A1ndez/8c18aac3a5207bb98ce80d106f449128fdc6cd55},
  urldate = {2024-09-11},
  abstract = {(Knowledge Extraction based onEvolutionary Learning) tool, an open source software that supports datamanagement and a designer of experiments. KEEL pays special attentionto the implementation of evolutionary learning and soft computing basedtechniques for Data Mining problems including regression, classification,clustering, pattern mining and so on.The aim of this paper is to present three new aspects of KEEL: KEEL-dataset, a data set repository which includes the data set partitions in theKEELformatandshowssomeresultsofalgorithmsinthesedatasets; someguidelines for including new algorithms in KEEL, helping the researcherstomaketheirmethodseasilyaccessibletootherauthorsandtocomparetheresults of many approaches already included within the KEEL software;and a module of statistical procedures developed in order to provide to theresearcher a suitable tool to contrast the results obtained in any experimen-talstudy.Acaseofstudyisgiventoillustrateacompletecaseofapplicationwithin this experimental analysis framework.}
}

@article{amelioNormalizedMutualInformation2015,
  title = {Is {{Normalized Mutual Information}} a {{Fair Measure}} for {{Comparing Community Detection Methods}}?},
  author = {Amelio, Alessia and Pizzuti, Clara},
  date = {2015-08-25},
  journaltitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
  pages = {1584--1585},
  publisher = {ACM},
  location = {Paris France},
  doi = {10.1145/2808797.2809344},
  url = {https://dl.acm.org/doi/10.1145/2808797.2809344},
  urldate = {2024-09-11},
  abstract = {Normalized mutual information (NMI) is a widely used measure to compare community detection methods. Recently, however, the need of adjustment for information theoretic based measures has been argued because of their tendency in choosing clustering solutions with more communities. In this paper an experimental evaluation is performed to investigate this problem, and an adjustment that scales the values of NMI is proposed. Experiments on synthetic generated networks highlight the unbiased behavior of scaled NMI.},
  eventtitle = {{{ASONAM}} '15: {{Advances}} in {{Social Networks Analysis}} and {{Mining}} 2015},
  isbn = {9781450338547},
  langid = {english}
}

@article{anwarMeasurementDataComplexity2014,
  title = {Measurement of Data Complexity for Classification Problems with Unbalanced Data},
  author = {Anwar, Nafees and Jones, Geoff and Ganesh, Siva},
  date = {2014},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {7},
  number = {3},
  pages = {194--211},
  issn = {1932-1872},
  doi = {10.1002/sam.11228},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11228},
  urldate = {2024-09-11},
  abstract = {We introduce a complexity measure for classification problems that takes account of deterioration in classifier performance as a result of class imbalance. The measure is based on k-nearest neighbors. We explore the choices of k and the distance metric through a simulation study, and illustrate the use of our measure, and related data visualization techniques, with real datasets from the literature.},
  langid = {english},
  keywords = {Bayes error,class imbalance,complexity measurement,nearest neighbors},
  file = {/Users/alexander/Zotero/storage/V8VGU9UG/Anwar et al. - 2014 - Measurement of data complexity for classification problems with unbalanced data.pdf;/Users/alexander/Zotero/storage/PRQGUTIV/sam.html}
}

@inproceedings{arthurKmeansAdvantagesCareful2007,
  title = {K-Means++: The Advantages of Careful Seeding},
  shorttitle = {K-Means++},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arthur, David and Vassilvitskii, Sergei},
  date = {2007-01-07},
  series = {{{SODA}} '07},
  pages = {1027--1035},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {USA},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  isbn = {978-0-89871-624-5}
}

@article{barellaAssessingDataComplexity2021,
  title = {Assessing the Data Complexity of Imbalanced Datasets},
  author = {Barella, Victor H. and Garcia, Luís P. F. and family=Souto, given=Marcilio C. P., prefix=de, useprefix=true and Lorena, Ana C. and family=Carvalho, given=André C. P. L. F., prefix=de, useprefix=true},
  date = {2021-04-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {553},
  pages = {83--109},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2020.12.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025520311713},
  urldate = {2024-09-11},
  abstract = {Imbalanced datasets are an important challenge in supervised Machine Learning (ML). According to the literature, class imbalance does not necessarily impose difficulties for ML algorithms. Difficulties mainly arise from other characteristics, such as overlapping between classes and complex decision boundaries. For binary classification tasks, calculating imbalance is straightforward, e.g., the ratio between class sizes. However, measuring more relevant characteristics, such as class overlapping, is not trivial. In the past years, complexity measures able to assess more relevant dataset characteristics have been proposed. In this paper, we investigate their effectiveness on real imbalanced datasets and how they are affected by applying different data imbalance treatments (DIT). For such, we perform two data-driven experiments: (1) We adapt the complexity measures to the context of imbalanced datasets. The experimental results show that our proposed measures assess the difficulty of imbalanced problems better than the original ones. We also compare the results with the state-of-art on data complexity measures for imbalanced datasets. (2) We analyze the behavior of complexity measures before and after applying DITs. According to the results, the difference in data complexity, in general, correlates to the predictive performance improvement obtained by applying DITs to the original datasets.},
  keywords = {Classification,Complexity measures,Imbalanced dataset,Pre-processing techniques},
  file = {/Users/alexander/Zotero/storage/H87IN2R3/Barella et al. - 2021 - Assessing the data complexity of imbalanced datasets.pdf;/Users/alexander/Zotero/storage/W7HYI8S2/S0020025520311713.html}
}

@inproceedings{batistaBalancingTrainingData2003,
  title = {Balancing {{Training Data}} for {{Automated Annotation}} of {{Keywords}}: A {{Case Study}}.},
  shorttitle = {Balancing {{Training Data}} for {{Automated Annotation}} of {{Keywords}}},
  author = {Batista, Gustavo and Bazzan, Ana and Monard, Maria-Carolina},
  date = {2003-01-01},
  pages = {10--18},
  abstract = {There has been an increasing interest in tools for automating the annotation of databases. Machine learning techniques are promising candidates to help curators to, at least, guide the process of annotation which is mostly done manually. Following previous works on automated annotation using symbolic machine learning techniques, the present work deals with a common problem in machine learning: that classes usually have skewed class prior probabilities, i.e., there is a large number of examples of one class compared with just few examples of the other class. This happens due to the fact that a large number of proteins is not annotated for every feature. Thus, we analyze and employ some techniques aiming at balancing the training data. Our experiments show that the classifiers induced from balanced data sampled with our method are more accurate than those induced from the original data.},
  eventtitle = {The {{Proc}}. {{Of Workshop}} on {{Bioinformatics}}},
  file = {/Users/alexander/Zotero/storage/9E9YFMPA/Batista et al. - 2003 - Balancing Training Data for Automated Annotation of Keywords a Case Study..pdf}
}

@article{batistaStudyBehaviorSeveral2004,
  title = {A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data},
  author = {Batista, Gustavo E. A. P. A. and Prati, Ronaldo C. and Monard, Maria Carolina},
  date = {2004-06},
  journaltitle = {ACM SIGKDD Explorations Newsletter},
  shortjournal = {SIGKDD Explor. Newsl.},
  volume = {6},
  number = {1},
  pages = {20--29},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/1007730.1007735},
  url = {https://dl.acm.org/doi/10.1145/1007730.1007735},
  urldate = {2024-09-11},
  abstract = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.},
  langid = {english}
}

@article{brancoPreprocessingApproachesImbalanced2019,
  title = {Pre-Processing Approaches for Imbalanced Distributions in Regression},
  author = {Branco, Paula and Torgo, Luis and Ribeiro, Rita P.},
  date = {2019-05},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {343},
  pages = {76--99},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.11.100},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219301638},
  urldate = {2024-09-11},
  langid = {english}
}

@article{brancoSurveyPredictiveModeling2016,
  title = {A {{Survey}} of {{Predictive Modeling}} on {{Imbalanced Domains}}},
  author = {Branco, Paula and Torgo, Luís and Ribeiro, Rita},
  date = {2016-08-01},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Computing Surveys},
  volume = {49},
  doi = {10.1145/2907070},
  abstract = {Many real-world data-mining applications involve obtaining predictive models using datasets with strongly imbalanced distributions of the target variable. Frequently, the least-common values of this target variable are associated with events that are highly relevant for end users (e.g., fraud detection, unusual returns on stock markets, anticipation of catastrophes, etc.). Moreover, the events may have different costs and benefits, which, when associated with the rarity of some of them on the available training data, creates serious problems to predictive modeling techniques. This article presents a survey of existing techniques for handling these important applications of predictive analytics. Although most of the existing work addresses classification tasks (nominal target variables), we also describe methods designed to handle similar problems within regression tasks (numeric target variables). In this survey, we discuss the main challenges raised by imbalanced domains, propose a definition of the problem, describe the main approaches to these tasks, propose a taxonomy of the methods, summarize the conclusions of existing comparative studies as well as some theoretical analyses of some methods, and refer to some related problems within predictive modeling.},
  file = {/Users/alexander/Zotero/storage/JXPEYSQ3/Branco et al. - 2016 - A Survey of Predictive Modeling on Imbalanced Domains.pdf}
}

@book{brazdilMetalearningApplicationsData2008,
  title = {Metalearning: {{Applications}} to {{Data Mining}}},
  shorttitle = {Metalearning},
  author = {Brazdil, Pavel and Carrier, Christophe Giraud and Soares, Carlos and Vilalta, Ricardo},
  date = {2008-11-26},
  publisher = {Springer Science \& Business Media},
  abstract = {Metalearning is the study of principled methods that exploit metaknowledge to obtain efficient models and solutions by adapting machine learning and data mining processes. While the variety of machine learning and data mining techniques now available can, in principle, provide good model solutions, a methodology is still needed to guide the search for the most appropriate model in an efficient way. Metalearning provides one such methodology that allows systems to become more effective through experience. This book discusses several approaches to obtaining knowledge concerning the performance of machine learning and data mining algorithms. It shows how this knowledge can be reused to select, combine, compose and adapt both algorithms and models to yield faster, more effective solutions to data mining problems. It can thus help developers improve their algorithms and also develop learning systems that can improve themselves.  The book will be of interest to researchers and graduate students in the areas of machine learning, data mining and artificial intelligence.},
  isbn = {978-3-540-73262-4},
  langid = {english},
  pagetotal = {182},
  keywords = {Computers / Computer Vision & Pattern Recognition,Computers / Databases / Data Mining,Computers / Intelligence (AI) & Semantics,Computers / Optical Data Processing,Computers / Software Development & Engineering / General},
  file = {/Users/alexander/Zotero/storage/S7Z5XNXB/Brazdil et al. - 2009 - Metalearning Applications to Data Mining.pdf}
}

@inproceedings{burnaevInfluenceResamplingAccuracy2015,
  title = {Influence of Resampling on Accuracy of Imbalanced Classification},
  booktitle = {Eighth International Conference on Machine Vision ({{ICMV}} 2015)},
  author = {Burnaev, Evgeny and Erofeev, Pavel and Papanov, Artem},
  date = {2015},
  volume = {9875},
  pages = {423--427},
  publisher = {SPIE},
  file = {/Users/alexander/Zotero/storage/L5PYK8SZ/Burnaev et al. - 2015 - Influence of resampling on accuracy of imbalanced classification.pdf}
}

@article{calinskiDendriteMethodCluster1974,
  title = {A Dendrite Method for Cluster Analysis},
  author = {Calinski, T. and Harabasz, J.},
  date = {1974},
  journaltitle = {Communications in Statistics - Theory and Methods},
  shortjournal = {Comm. in Stats. - Theory \& Methods},
  volume = {3},
  number = {1},
  pages = {1--27},
  issn = {0361-0926},
  doi = {10.1080/03610927408827101},
  url = {http://www.tandfonline.com/doi/abs/10.1080/03610927408827101},
  urldate = {2024-09-11},
  langid = {english}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  date = {2002-06-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {16},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  url = {https://www.jair.org/index.php/jair/article/view/10302},
  urldate = {2024-09-11},
  abstract = {An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.},
  file = {/Users/alexander/Zotero/storage/L85PBUTZ/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf}
}

@online{ClusteringEvaluationDaviesBouldin,
  title = {Clustering {{Evaluation}} by {{Davies-Bouldin Index}}({{DBI}}) in {{Cereal}} Data Using {{K-Means}}},
  url = {https://ieeexplore.ieee.org/document/9076555},
  urldate = {2024-09-11},
  abstract = {Cereals grains have been used as a principle ingredient of human diet for hundreds of years. Indian cereal crops provide vital nutrients and energy to the human diet. The motivation behind this research paper is to distribute the research discoveries of applying K-Means clustering, on a cereal dataset and to differentiate the outcomes found on the number of bunches to identify whether the ideal or best number of groups to be 3 or 5. This speculation is achieved by applying distinctive clustering tests (likewise reordered in the paper), and visualizations. The aforementioned resolution by doing exploratory analysis, at that point modeled fitting followed by result testing, driving us to a definite end. The language utilized for our exploration is R.},
  langid = {american},
  file = {/Users/alexander/Zotero/storage/PSYKFWP2/9076555.html}
}

@article{cohenCoefficientAgreementNominal1960,
  title = {A {{Coefficient}} of {{Agreement}} for {{Nominal Scales}}},
  author = {Cohen, Jacob},
  date = {1960-04},
  journaltitle = {Educational and Psychological Measurement},
  shortjournal = {Educational and Psychological Measurement},
  volume = {20},
  number = {1},
  pages = {37--46},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/001316446002000104},
  url = {http://journals.sagepub.com/doi/10.1177/001316446002000104},
  urldate = {2024-09-11},
  langid = {english}
}

@online{dasguptaCausalReasoningMetareinforcement2019,
  title = {Causal {{Reasoning}} from {{Meta-reinforcement Learning}}},
  author = {Dasgupta, Ishita and Wang, Jane and Chiappa, Silvia and Mitrovic, Jovana and Ortega, Pedro and Raposo, David and Hughes, Edward and Battaglia, Peter and Botvinick, Matthew and Kurth-Nelson, Zeb},
  date = {2019-01-23},
  eprint = {1901.08162},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.08162},
  urldate = {2024-09-11},
  abstract = {Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether causal reasoning can emerge via meta-reinforcement learning. We train a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure. We find that the trained agent can perform causal reasoning in novel situations in order to obtain rewards. The agent can select informative interventions, draw causal inferences from observational data, and make counterfactual predictions. Although established formal causal reasoning algorithms also exist, in this paper we show that such reasoning can arise from model-free reinforcement learning, and suggest that causal reasoning in complex settings may benefit from the more end-to-end learning-based approaches presented here. This work also offers new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform -- and interpret -- experiments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexander/Zotero/storage/99RDRN4X/Dasgupta et al. - 2019 - Causal Reasoning from Meta-reinforcement Learning.pdf;/Users/alexander/Zotero/storage/3QKPBUAJ/1901.html}
}

@inproceedings{duanOneShotImitationLearning2017,
  title = {One-{{Shot Imitation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Jonathan Ho, OpenAI and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/ba3866600c3540f67c1e9575e213be0a-Abstract.html},
  urldate = {2024-09-11},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.  Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations.  For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states.  At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks.  A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.},
  file = {/Users/alexander/Zotero/storage/SLC9UURN/Duan et al. - 2017 - One-Shot Imitation Learning.pdf}
}

@article{fowlkesMethodComparingTwo1983,
  title = {A {{Method}} for {{Comparing Two Hierarchical Clusterings}}},
  author = {Fowlkes, E. B. and Mallows, C. L.},
  date = {1983-09},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {78},
  number = {383},
  pages = {553--569},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1983.10478008},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008},
  urldate = {2024-09-11},
  langid = {english}
}

@article{geHandlingNegativeTransfer2014,
  title = {On Handling Negative Transfer and Imbalanced Distributions in Multiple Source Transfer Learning},
  author = {Ge, Liang and Gao, Jing and Ngo, Hung and Li, Kang and Zhang, Aidong},
  date = {2014-08},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  shortjournal = {Statistical Analysis},
  volume = {7},
  number = {4},
  pages = {254--271},
  issn = {1932-1864, 1932-1872},
  doi = {10.1002/sam.11217},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sam.11217},
  urldate = {2024-09-11},
  abstract = {Abstract                            Transfer learning has benefited many real‐world applications where labeled data are abundant in source domains but scarce in the target domain. As there are usually multiple relevant domains where knowledge can be transferred, multiple source transfer learning (               MSTL               ) has recently attracted much attention. However, we are facing two major challenges when applying               MSTL               . First, without knowledge about the difference between source and target domains,               negative transfer               occurs when knowledge is transferred from highly irrelevant sources. Second, existence of               imbalanced distributions               in classes, where examples in one class dominate, can lead to improper judgement on the source domains' relevance to the target task. Since existing               MSTL               methods are usually designed to transfer from relevant sources with balanced distributions, they will fail in applications where these two challenges persist. In this article, we propose a novel two‐phase framework to effectively transfer knowledge from multiple sources even when there exists irrelevant sources and imbalanced class distributions. First, an effective supervised local weight scheme is proposed to assign a proper weight to each source domain's classifier based on its ability of predicting accurately on each local region of the target domain. The second phase then learns a classifier for the target domain by solving an optimization problem which concerns both training error minimization and consistency with weighted predictions gained from source domains. A theoretical analysis shows that as the number of source domains increases, the probability that the proposed approach has an error greater than a bound is becoming exponentially small. We further extend the proposed approach to an online processing scenario to conduct transfer learning on continuously arriving data. Extensive experiments on disease prediction, spam filtering and intrusion detection datasets demonstrate that: (i) the proposed two‐phase approach outperforms existing               MSTL               approaches due to its ability of tackling negative transfer and imbalanced distribution challenges, and (ii) the proposed online approach achieves comparable performance to the offline scheme.},
  langid = {english}
}

@inproceedings{gordonVersaVersatileEfficient2018,
  title = {Versa: {{Versatile}} and {{Efficient Few-shot Learning}}},
  shorttitle = {Versa},
  author = {Gordon, Jonathan and Bronskill, J. and Bauer, M. and Nowozin, Sebastian and Turner, Richard E.},
  date = {2018},
  url = {https://www.semanticscholar.org/paper/Versa%3A-Versatile-and-Efficient-Few-shot-Learning-Gordon-Bronskill/f24e03e40fa2141f67c6994da5710b596054c74e},
  urldate = {2024-09-11},
  abstract = {Despite recent advances in few-shot learning, notably in meta-learning based approaches [Ravi and Larochelle, 2017, Vinyals et al., 2016, Edwards and Storkey, 2017, Finn et al., 2017, Lacoste et al., 2018], there remains a lack of general purpose methods for flexible, data-efficient learning. This paper introduces VERSA, a system for data efficient and versatile meta-learning. It employs a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. We evaluate VERSA on benchmark datasets where the method achieves state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.},
  file = {/Users/alexander/Zotero/storage/SWP8EKGS/Gordon et al. - 2018 - Versa Versatile and Efficient Few-shot Learning.pdf}
}

@inproceedings{haiboheADASYNAdaptiveSynthetic2008,
  title = {{{ADASYN}}: {{Adaptive}} Synthetic Sampling Approach for Imbalanced Learning},
  shorttitle = {{{ADASYN}}},
  booktitle = {2008 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IEEE World Congress}} on {{Computational Intelligence}})},
  author = {{Haibo He} and {Yang Bai} and Garcia, Edwardo A. and {Shutao Li}},
  date = {2008-06},
  pages = {1322--1328},
  publisher = {IEEE},
  location = {Hong Kong, China},
  doi = {10.1109/IJCNN.2008.4633969},
  url = {http://ieeexplore.ieee.org/document/4633969/},
  urldate = {2024-09-11},
  eventtitle = {2008 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2008 - {{Hong Kong}})},
  isbn = {978-1-4244-1820-6},
  file = {/Users/alexander/Zotero/storage/GD924GA2/Haibo He et al. - 2008 - ADASYN Adaptive synthetic sampling approach for imbalanced learning.pdf}
}

@incollection{hanBorderlineSMOTENewSampling2005,
  title = {Borderline-{{SMOTE}}: {{A New Over-Sampling Method}} in {{Imbalanced Data Sets Learning}}},
  shorttitle = {Borderline-{{SMOTE}}},
  booktitle = {Advances in {{Intelligent Computing}}},
  author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
  editor = {Huang, De-Shuang and Zhang, Xiao-Ping and Huang, Guang-Bin},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2005},
  volume = {3644},
  pages = {878--887},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11538059_91},
  url = {http://link.springer.com/10.1007/11538059_91},
  urldate = {2024-09-11},
  isbn = {978-3-540-28226-6 978-3-540-31902-3}
}

@article{hartCondensedNearestNeighbor1968,
  title = {The Condensed Nearest Neighbor Rule ({{Corresp}}.)},
  author = {Hart, P.},
  date = {1968-05},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {14},
  number = {3},
  pages = {515--516},
  issn = {0018-9448},
  doi = {10.1109/TIT.1968.1054155},
  url = {http://ieeexplore.ieee.org/document/1054155/},
  urldate = {2024-09-11},
  langid = {english}
}

@article{hayesAnsweringCallStandard2007,
  title = {Answering the {{Call}} for a {{Standard Reliability Measure}} for {{Coding Data}}},
  author = {Hayes, Andrew F. and Krippendorff, Klaus},
  date = {2007-04-01},
  journaltitle = {Communication Methods and Measures},
  volume = {1},
  number = {1},
  pages = {77--89},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312450709336664},
  url = {https://doi.org/10.1080/19312450709336664},
  urldate = {2024-09-11},
  abstract = {In content analysis and similar methods, data are typically generated by trained human observers who record or transcribe textual, pictorial, or audible matter in terms suitable for analysis. Conclusions from such data can be trusted only after demonstrating their reliability. Unfortunately, the content analysis literature is full of proposals for so-called reliability coefficients, leaving investigators easily confused, not knowing which to choose. After describing the criteria for a good measure of reliability, we propose Krippendorff's alpha as the standard reliability measure. It is general in that it can be used regardless of the number of observers, levels of measurement, sample sizes, and presence or absence of missing data. To facilitate the adoption of this recommendation, we describe a freely available macro written for SPSS and SAS to calculate Krippendorff's alpha and illustrate its use with a simple example.}
}

@article{heImbalancedLearning2009,
  title = {Learning from Imbalanced Data},
  author = {He, Haibo and Garcia, Edwardo A.},
  date = {2009},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {21},
  number = {9},
  pages = {1263--1284},
  doi = {10.1109/TKDE.2008.239},
  keywords = {active learning,assessment metrics.,Availability,classification,cost-sensitive learning,Data analysis,Data engineering,Data security,Decision making,Finance,Imbalanced learning,IP networks,kernel-based learning,Knowledge representation,Large-scale systems,sampling methods,Surveillance},
  file = {/Users/alexander/Zotero/storage/VWFX5UP5/He and Garcia - 2009 - Learning from imbalanced data.pdf}
}

@article{huMaliciousWebDomain2019,
  title = {Malicious Web Domain Identification Using Online Credibility and Performance Data by Considering the Class Imbalance Issue},
  author = {Hu, Zhongyi and Chiong, Raymond and Pranata, Ilung and Bao, Yukun and Lin, Yuqing},
  date = {2019-01-01},
  journaltitle = {Industrial Management \& Data Systems},
  volume = {119},
  number = {3},
  pages = {676--696},
  publisher = {Emerald Publishing Limited},
  issn = {0263-5577},
  doi = {10.1108/IMDS-02-2018-0072},
  url = {https://doi.org/10.1108/IMDS-02-2018-0072},
  urldate = {2024-09-10},
  abstract = {Purpose Malicious web domain identification is of significant importance to the security protection of internet users. With online credibility and performance data, the purpose of this paper to investigate the use of machine learning techniques for malicious web domain identification by considering the class imbalance issue (i.e. there are more benign web domains than malicious ones). Design/methodology/approach The authors propose an integrated resampling approach to handle class imbalance by combining the synthetic minority oversampling technique (SMOTE) and particle swarm optimisation (PSO), a population-based meta-heuristic algorithm. The authors use the SMOTE for oversampling and PSO for undersampling. Findings By applying eight well-known machine learning classifiers, the proposed integrated resampling approach is comprehensively examined using several imbalanced web domain data sets with different imbalance ratios. Compared to five other well-known resampling approaches, experimental results confirm that the proposed approach is highly effective. Practical implications This study not only inspires the practical use of online credibility and performance data for identifying malicious web domains but also provides an effective resampling approach for handling the class imbalance issue in the area of malicious web domain identification. Originality/value Online credibility and performance data are applied to build malicious web domain identification models using machine learning techniques. An integrated resampling approach is proposed to address the class imbalance issue. The performance of the proposed approach is confirmed based on real-world data sets with different imbalance ratios.},
  keywords = {Credibility and performance,Imbalance class distribution,Information security,Internet users,Malicious web domain,Online data,Particle swarm optimization,Synthetic minority oversampling technique},
  file = {/Users/alexander/Zotero/storage/G4LER9YI/Hu et al. - 2019 - Malicious web domain identification using online credibility and performance data by considering the.pdf;/Users/alexander/Zotero/storage/YIUXJ2GH/html.html}
}

@online{humplikMetaReinforcementLearning2019,
  title = {Meta Reinforcement Learning as Task Inference},
  author = {Humplik, Jan and Galashov, Alexandre and Hasenclever, Leonard and Ortega, Pedro A. and Teh, Yee Whye and Heess, Nicolas},
  date = {2019-10-22},
  eprint = {1905.06424},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.06424},
  urldate = {2024-09-11},
  abstract = {Humans achieve efficient learning by relying on prior knowledge about the structure of naturally occurring tasks. There is considerable interest in designing reinforcement learning (RL) algorithms with similar properties. This includes proposals to learn the learning algorithm itself, an idea also known as meta learning. One formal interpretation of this idea is as a partially observable multi-task RL problem in which task information is hidden from the agent. Such unknown task problems can be reduced to Markov decision processes (MDPs) by augmenting an agent's observations with an estimate of the belief about the task based on past experience. However estimating the belief state is intractable in most partially-observed MDPs. We propose a method that separately learns the policy and the task belief by taking advantage of various kinds of privileged information. Our approach can be very effective at solving standard meta-RL environments, as well as a complex continuous control environment with sparse rewards and requiring long-term memory.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexander/Zotero/storage/4WQX6GGY/Humplik et al. - 2019 - Meta reinforcement learning as task inference.pdf;/Users/alexander/Zotero/storage/HC8PK6VJ/1905.html}
}

@online{jaafraReviewMetaReinforcementLearning2018,
  title = {A {{Review}} of {{Meta-Reinforcement Learning}} for {{Deep Neural Networks Architecture Search}}},
  author = {Jaafra, Yesmina and Laurent, Jean Luc and Deruyver, Aline and Naceur, Mohamed Saber},
  date = {2018-12-17},
  eprint = {1812.07995},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.07995},
  urldate = {2024-09-11},
  abstract = {Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and doesn't always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexander/Zotero/storage/X63ANRI9/Jaafra et al. - 2018 - A Review of Meta-Reinforcement Learning for Deep Neural Networks Architecture Search.pdf;/Users/alexander/Zotero/storage/HP6R5UU3/1812.html}
}

@online{jerfelReconcilingMetalearningContinual2019,
  title = {Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks},
  author = {Jerfel, Ghassen and Grant, Erin and Griffiths, Thomas L. and Heller, Katherine},
  date = {2019-06-19},
  eprint = {1812.06080},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.06080},
  urldate = {2024-09-11},
  abstract = {Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexander/Zotero/storage/YVNTQ2DZ/Jerfel et al. - 2019 - Reconciling meta-learning and continual learning with online mixtures of tasks.pdf;/Users/alexander/Zotero/storage/844TPDIG/1812.html}
}

@article{jiangMultiSubspaceStructuredMetaLearning2021,
  title = {Multi-{{Subspace Structured Meta-Learning}}},
  author = {Jiang, Weisen and Kwok, James and Zhang, Yu},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=C_RTGckbu-A},
  urldate = {2024-09-11},
  abstract = {Meta-learning aims to extract meta-knowledge from historical tasks to accelerate learning on new tasks. A critical challenge in meta-learning is to handle task heterogeneity, i.e., tasks lie in different distributions. Unlike typical meta-learning algorithms that learn a globally shared initialization, recent structured meta-learning algorithms formulate tasks into multiple groups and learn an initialization for tasks in each group using centroid-based clustering. However, those algorithms still require task models in the same group to be close together and fail to take advantage of negative correlations between tasks. In this paper, task models are formulated into a subspace structure. We propose a MUlti-Subspace structured Meta-Learning (MUSML) algorithm to learn the subspace bases. We establish the convergence and analyze the generalization performance. Experimental results confirm the effectiveness of the proposed MUSML algorithm.},
  langid = {english},
  file = {/Users/alexander/Zotero/storage/9383Z2FZ/Jiang et al. - 2021 - Multi-Subspace Structured Meta-Learning.pdf}
}

@article{kangGRATISGeneRAtingTIme2020,
  title = {{{GRATIS}}: {{GeneRAting TIme Series}} with Diverse and Controllable Characteristics},
  shorttitle = {{{GRATIS}}},
  author = {Kang, Yanfei and Hyndman, Rob J. and Li, Feng},
  date = {2020-08},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  shortjournal = {Statistical Analysis},
  volume = {13},
  number = {4},
  pages = {354--376},
  issn = {1932-1864, 1932-1872},
  doi = {10.1002/sam.11461},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sam.11461},
  urldate = {2024-09-11},
  abstract = {Abstract             The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We simulate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application.},
  langid = {english},
  file = {/Users/alexander/Zotero/storage/YCKES8AK/Kang et al. - 2020 - GRATIS GeneRAting TIme Series with diverse and controllable characteristics.pdf}
}

@inproceedings{kaoshikACLSQLGeneratingSQL2021,
  title = {{{ACL-SQL}}: {{Generating SQL Queries}} from {{Natural Language}}},
  shorttitle = {{{ACL-SQL}}},
  booktitle = {Proceedings of the 3rd {{ACM India Joint International Conference}} on {{Data Science}} \& {{Management}} of {{Data}} (8th {{ACM IKDD CODS}} \& 26th {{COMAD}})},
  author = {Kaoshik, Ronak and Patil, Rohit and R, Prakash and Agarawal, Shaurya and Jain, Naman and Singh, Mayank},
  date = {2021-01-02},
  pages = {423--423},
  publisher = {ACM},
  location = {Bangalore India},
  doi = {10.1145/3430984.3431046},
  url = {https://dl.acm.org/doi/10.1145/3430984.3431046},
  urldate = {2024-09-11},
  eventtitle = {{{CODS COMAD}} 2021: 8th {{ACM IKDD CODS}} and 26th {{COMAD}}},
  isbn = {978-1-4503-8817-7},
  langid = {english}
}

@article{kaurSystematicReviewImbalanced2019,
  title = {A {{Systematic Review}} on {{Imbalanced Data Challenges}} in {{Machine Learning}}: {{Applications}} and {{Solutions}}},
  shorttitle = {A {{Systematic Review}} on {{Imbalanced Data Challenges}} in {{Machine Learning}}},
  author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
  date = {2019-08-30},
  journaltitle = {ACM Comput. Surv.},
  volume = {52},
  number = {4},
  pages = {79:1--79:36},
  issn = {0360-0300},
  doi = {10.1145/3343440},
  url = {https://dl.acm.org/doi/10.1145/3343440},
  urldate = {2024-09-10},
  abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
  file = {/Users/alexander/Zotero/storage/VSXDNB4D/Kaur et al. - 2019 - A Systematic Review on Imbalanced Data Challenges in Machine Learning Applications and Solutions.pdf}
}

@article{kubatAddressingCurseImbalanced2000,
  title = {Addressing the {{Curse}} of {{Imbalanced Training Sets}}: {{One-Sided Selection}}},
  shorttitle = {Addressing the {{Curse}} of {{Imbalanced Training Sets}}},
  author = {Kubat, M.},
  date = {2000-06-25},
  journaltitle = {Fourteenth International Conference on Machine Learning},
  shortjournal = {Fourteenth International Conference on Machine Learning},
  abstract = {Adding examples of the majority class to the training set can have a detrimental effect on the learner's behavior: noisy or otherwise unreliable examples from the majority class can overwhelm the minority class. The paper discusses criteria to evaluate the utility of classifiers induced from such imbalanced training sets, gives explanation of the poor behavior of some learners under these circumstances, and suggests as a solution a simple technique called one-sided selection of examples. 1 Introduction The general topic of this paper is learning from examples described by pairs [(x; c(x)], where x is a vector of attribute values and c(x) is the corresponding concept label. For simplicity, we consider only problems where c(x) is either positive or negative, and all attributes are continuous. Since Fisher (1936), this task has received plenty of attention from statisticians as well as from researchers in artificial neural networks, AI, and ML. A typical scenario assumes the e...},
  file = {/Users/alexander/Zotero/storage/FU7HAT38/Kubat - 2000 - Addressing the Curse of Imbalanced Training Sets One-Sided Selection.pdf}
}

@incollection{laurikkalaImprovingIdentificationDifficult2001,
  title = {Improving {{Identification}} of {{Difficult Small Classes}} by {{Balancing Class Distribution}}},
  booktitle = {Artificial {{Intelligence}} in {{Medicine}}},
  author = {Laurikkala, Jorma},
  editor = {Quaglini, Silvana and Barahona, Pedro and Andreassen, Steen},
  editora = {Goos, G. and Hartmanis, J. and Van Leeuwen, J.},
  editoratype = {redactor},
  date = {2001},
  volume = {2101},
  pages = {63--66},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-48229-6_9},
  url = {http://link.springer.com/10.1007/3-540-48229-6_9},
  urldate = {2024-09-11},
  isbn = {978-3-540-42294-5 978-3-540-48229-1},
  file = {/Users/alexander/Zotero/storage/BZMH8C7A/Laurikkala - 2001 - Improving Identification of Difficult Small Classes by Balancing Class Distribution.pdf}
}

@online{leeDualAttentionTime2020,
  title = {Dual {{Attention}} in {{Time}} and {{Frequency Domain}} for {{Voice Activity Detection}}},
  author = {Lee, Joohyung and Jung, Youngmoon and Kim, Hoirin},
  date = {2020-08-25},
  eprint = {2003.12266},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2003.12266},
  url = {http://arxiv.org/abs/2003.12266},
  urldate = {2024-09-10},
  abstract = {Voice activity detection (VAD) is a challenging task in low signal-to-noise ratio (SNR) environment, especially in non-stationary noise. To deal with this issue, we propose a novel attention module that can be integrated in Long Short-Term Memory (LSTM). Our proposed attention module refines each LSTM layer's hidden states so as to make it possible to adaptively focus on both time and frequency domain. Experiments are conducted on various noisy conditions using Aurora 4 database. Our proposed method obtains the 95.58 \% area under the ROC curve (AUC), achieving 22.05 \% relative improvement compared to baseline, with only 2.44 \% increase in the number of parameters. Besides, we utilize focal loss for alleviating the performance degradation caused by imbalance between speech and non-speech sections in training sets. The results show that the focal loss can improve the performance in various imbalance situations compared to the cross entropy loss, a commonly used loss function in VAD.},
  pubstate = {prepublished},
  keywords = {Audio and Speech Processing (eess.AS),Electrical Engineering and Systems Science - Audio and Speech Processing,FOS: Electrical engineering electronic engineering information engineering},
  file = {/Users/alexander/Zotero/storage/XQCGJ8S9/Lee et al. - 2020 - Dual Attention in Time and Frequency Domain for Voice Activity Detection.pdf;/Users/alexander/Zotero/storage/IA2EEACB/2003.html}
}

@article{leiInnovativeApproachBased2023,
  title = {An Innovative Approach Based on Meta-Learning for Real-Time Modal Fault Diagnosis with Small Sample Learning},
  author = {Lei, Tongfei and Hu, Jiabei and Riaz, Saleem},
  date = {2023-07-03},
  journaltitle = {Frontiers in Physics},
  shortjournal = {Front. Phys.},
  volume = {11},
  pages = {1207381},
  issn = {2296-424X},
  doi = {10.3389/fphy.2023.1207381},
  url = {https://www.frontiersin.org/articles/10.3389/fphy.2023.1207381/full},
  urldate = {2024-09-11},
  abstract = {The actual multimodal process data usually exhibit non-linear time correlation and non-Gaussian distribution accompanied by new modes. Existing fault diagnosis methods have difficulty adapting to the complex nature of new modalities and are unable to train models based on small samples. Therefore, this paper proposes a new modal fault diagnosis method based on meta-learning (ML) and neural architecture search (NAS), MetaNAS. Specifically, the best performing network model of the existing modal is first automatically obtained using NAS, and then, the fault diagnosis model design is learned from the NAS of the existing model using ML. Finally, when generating new modalities, the gradient is updated based on the learned design experience, i.e., new modal fault diagnosis models are quickly generated under small sample conditions. The effectiveness and feasibility of the proposed method are fully verified by the numerical system and simulation experiments of the Tennessee Eastman (TE) chemical process.},
  file = {/Users/alexander/Zotero/storage/J7LQ59A5/Lei et al. - 2023 - An innovative approach based on meta-learning for real-time modal fault diagnosis with small sample.pdf}
}

@article{liAdaptiveKernelValue2019,
  title = {Adaptive {{Kernel Value Caching}} for {{SVM Training}}},
  author = {Li, Qinbin and Wen, Zeyi and He, Bingsheng},
  date = {2019},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  pages = {1--11},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2019.2944562},
  url = {https://ieeexplore.ieee.org/document/8890007/},
  urldate = {2024-09-11},
  file = {/Users/alexander/Zotero/storage/LRKJVRCB/Li et al. - 2019 - Adaptive Kernel Value Caching for SVM Training.pdf}
}

@online{luBayesImbalanceImpact2019,
  title = {Bayes {{Imbalance Impact Index}}: {{A Measure}} of {{Class Imbalanced Dataset}} for {{Classification Problem}}},
  shorttitle = {Bayes {{Imbalance Impact Index}}},
  author = {Lu, Yang and Cheung, Yiu-ming and Tang, Yuan Yan},
  date = {2019-01-29},
  eprint = {1901.10173},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.10173},
  url = {http://arxiv.org/abs/1901.10173},
  urldate = {2024-09-11},
  abstract = {Recent studies have shown that imbalance ratio is not the only cause of the performance loss of a classifier in imbalanced data classification. In fact, other data factors, such as small disjuncts, noises and overlapping, also play the roles in tandem with imbalance ratio, which makes the problem difficult. Thus far, the empirical studies have demonstrated the relationship between the imbalance ratio and other data factors only. To the best of our knowledge, there is no any measurement about the extent of influence of class imbalance on the classification performance of imbalanced data. Further, it is also unknown for a dataset which data factor is actually the main barrier for classification. In this paper, we focus on Bayes optimal classifier and study the influence of class imbalance from a theoretical perspective. Accordingly, we propose an instance measure called Individual Bayes Imbalance Impact Index (\$IBI\textasciicircum 3\$) and a data measure called Bayes Imbalance Impact Index (\$BI\textasciicircum 3\$). \$IBI\textasciicircum 3\$ and \$BI\textasciicircum 3\$ reflect the extent of influence purely by the factor of imbalance in terms of each minority class sample and the whole dataset, respectively. Therefore, \$IBI\textasciicircum 3\$ can be used as an instance complexity measure of imbalance and \$BI\textasciicircum 3\$ is a criterion to show the degree of how imbalance deteriorates the classification. As a result, we can therefore use \$BI\textasciicircum 3\$ to judge whether it is worth using imbalance recovery methods like sampling or cost-sensitive methods to recover the performance loss of a classifier. The experiments show that \$IBI\textasciicircum 3\$ is highly consistent with the increase of prediction score made by the imbalance recovery methods and \$BI\textasciicircum 3\$ is highly consistent with the improvement of F1 score made by the imbalance recovery methods on both synthetic and real benchmark datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexander/Zotero/storage/EQECBDQY/Lu et al. - 2019 - Bayes Imbalance Impact Index A Measure of Class Imbalanced Dataset for Classification Problem.pdf;/Users/alexander/Zotero/storage/5SPLY6B8/1901.html}
}

@article{makkiImbalancedFraud2019,
  title = {An Experimental Study with Imbalanced Classification Approaches for Credit Card Fraud Detection},
  author = {Makki, Sara and Assaghir, Zainab and Taher, Yehia and Haque, Rafiqul and Hacid, Mohand-Saïd and Zeineddine, Hassan},
  date = {2019},
  journaltitle = {IEEE access : practical innovations, open solutions},
  shortjournal = {IEEE Access},
  volume = {7},
  pages = {93010--93022},
  doi = {10.1109/ACCESS.2019.2927266},
  keywords = {Classification algorithms,Credit cards,Decision trees,Fraud analysis and detection,fraud cybercrimes,Hidden Markov models,imbalanced classification,Machine learning algorithms,Neural networks,secure society,Support vector machines},
  file = {/Users/alexander/Zotero/storage/34DPMVKD/Makki et al. - 2019 - An experimental study with imbalanced classification approaches for credit card fraud detection.pdf}
}

@inproceedings{maniKNNApproachUnbalanced2003a,
  title = {{{kNN}} Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction},
  shorttitle = {{{kNN}} Approach to Unbalanced Data Distributions},
  booktitle = {Proceedings of Workshop on Learning from Imbalanced Datasets},
  author = {Mani, Inderjeet and Zhang, I.},
  date = {2003},
  volume = {126},
  number = {1},
  pages = {1--7},
  publisher = {ICML},
  url = {https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf},
  urldate = {2024-09-11},
  file = {/Users/alexander/Zotero/storage/WW49DPMF/Mani and Zhang - 2003 - kNN approach to unbalanced data distributions a case study involving information extraction.pdf}
}

@article{menardiTrainingAssessingClassification2014,
  title = {Training and Assessing Classification Rules with Imbalanced Data},
  author = {Menardi, Giovanna and Torelli, Nicola},
  date = {2014-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {28},
  number = {1},
  pages = {92--122},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-012-0295-5},
  url = {http://link.springer.com/10.1007/s10618-012-0295-5},
  urldate = {2024-09-11},
  langid = {english}
}

@inproceedings{mittalClusteringEvaluationDaviesBouldin2020,
  title = {Clustering {{Evaluation}} by {{Davies-Bouldin Index}}({{DBI}}) in {{Cereal}} Data Using {{K-Means}}},
  author = {Mittal, Shantanu and Malhotra, Prashant and Srivastava, Yash},
  date = {2020-03-01},
  pages = {306--310},
  doi = {10.1109/ICCMC48092.2020.ICCMC-00057}
}

@article{monizAutomatedImbalancedClassification2021,
  title = {Automated Imbalanced Classification via Meta-Learning},
  author = {Moniz, Nuno and Cerqueira, Vitor},
  date = {2021-09-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {178},
  pages = {115011},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.115011},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417421004528},
  urldate = {2024-09-10},
  abstract = {Imbalanced learning is one of the most relevant problems in machine learning. However, it faces two crucial challenges. First, the amount of methods proposed to deal with such problem has grown immensely, making the validation of a large set of methods impractical. Second, it requires specialised knowledge, hindering its use by those without such level of experience. In this paper, we propose the Automated Imbalanced Classification method, ATOMIC. Such a method is the first automated machine learning approach for imbalanced classification tasks. It provides a ranking of solutions most likely to ensure an optimal approximation to a new domain, drastically reducing associated computational complexity and energy consumption. We carry this out by anticipating the loss of a large set of predictive solutions in new imbalanced learning tasks. We compare the predictive performance of ATOMIC against state-of-the-art methods using 101 imbalanced data sets. Results demonstrate that the proposed method provides a relevant approach to imbalanced learning while reducing learning and testing efforts of candidate solutions by approximately 95\%.},
  keywords = {Automated machine learning,Classification,Imbalance domain learning,Meta-learning},
  file = {/Users/alexander/Zotero/storage/YEKSD2TN/S0957417421004528.html}
}

@article{nathanielMetaFluxMetalearningGlobal2023,
  title = {{{MetaFlux}}: {{Meta-learning}} Global Carbon Fluxes from Sparse Spatiotemporal Observations},
  shorttitle = {{{MetaFlux}}},
  author = {Nathaniel, Juan and Liu, Jiangong and Gentine, Pierre},
  date = {2023-07-11},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {10},
  number = {1},
  pages = {440},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-02349-y},
  url = {https://www.nature.com/articles/s41597-023-02349-y},
  urldate = {2024-09-11},
  abstract = {Abstract                            We provide a global, long-term carbon flux dataset of gross primary production and ecosystem respiration generated using meta-learning, called               MetaFlux               . The idea behind meta-learning stems from the need to learn efficiently given sparse data by learning how to learn broad features across tasks to better infer other poorly sampled ones. Using meta-trained ensemble of deep models, we generate global carbon products on daily and monthly timescales at a 0.25-degree spatial resolution from 2001 to 2021, through a combination of reanalysis and remote-sensing products. Site-level validation finds that MetaFlux ensembles have lower validation error by 5–7\% compared to their non-meta-trained counterparts. In addition, they are more robust to extreme observations, with 4–24\% lower errors. We also checked for seasonality, interannual variability, and correlation to solar-induced fluorescence of the upscaled product and found that MetaFlux outperformed other machine-learning based carbon product, especially in the tropics and semi-arids by 10–40\%. Overall, MetaFlux can be used to study a wide range of biogeochemical processes.},
  langid = {english},
  file = {/Users/alexander/Zotero/storage/AQNQQ5L4/Nathaniel et al. - 2023 - MetaFlux Meta-learning global carbon fluxes from sparse spatiotemporal observations.pdf}
}

@article{nguyenBorderlineSamplingImbalanced2011,
  title = {Borderline Over-Sampling for Imbalanced Data Classification},
  author = {Nguyen, Hien M. and Cooper, Eric W. and Kamei, Katsuari},
  date = {2011-04-01},
  journaltitle = {Int. J. Knowl. Eng. Soft Data Paradigm.},
  volume = {3},
  number = {1},
  pages = {4--21},
  issn = {1755-3210},
  doi = {10.1504/IJKESDP.2011.039875},
  url = {https://doi.org/10.1504/IJKESDP.2011.039875},
  urldate = {2024-09-11},
  abstract = {Traditional classification algorithms usually provide poor accuracy on the prediction of the minority class of imbalanced data sets. This paper proposes a new method for dealing with imbalanced data sets by over-sampling the borderline minority class instances. A Support Vector Machine (SVM) classifier is then trained to predict future instances. Compared with other over-sampling methods, the proposed method focuses only on the minority class instances residing along the decision boundary, due to the fact that this region is the most crucial for establishing the decision boundary. Furthermore, the artificial minority instances are generated in such a way that the regions of the minority class with fewer majority class instances would be expanded by extrapolation, otherwise the current boundary of the minority class would be consolidated by interpolation. Experimental results show that the proposed method achieves a better performance than other over-sampling methods.},
  file = {/Users/alexander/Zotero/storage/WANUQCHQ/Nguyen et al. - 2011 - Borderline over-sampling for imbalanced data classification.pdf}
}

@article{nguyenDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} for {{Multiagent Systems}}: {{A Review}} of {{Challenges}}, {{Solutions}}, and {{Applications}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Multiagent Systems}}},
  author = {Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
  date = {2020-09},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {50},
  number = {9},
  pages = {3826--3839},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2020.2977374},
  url = {https://ieeexplore.ieee.org/document/9043893/},
  urldate = {2024-09-11},
  file = {/Users/alexander/Zotero/storage/HABVS94E/Nguyen et al. - 2020 - Deep Reinforcement Learning for Multiagent Systems A Review of Challenges, Solutions, and Applicati.pdf}
}

@inproceedings{orozco-acostaBayesianModelingApproach2020,
  title = {Bayesian {{Modeling Approach}} in {{Big Data Contexts}}: An {{Application}} in {{Spatial Epidemiology}}},
  shorttitle = {Bayesian {{Modeling Approach}} in {{Big Data Contexts}}},
  booktitle = {2020 {{IEEE}} 7th {{International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  author = {Orozco-Acosta, Erick and Adin, Aritz and Ugarte, Maria Dolores},
  date = {2020-10},
  pages = {749--750},
  publisher = {IEEE},
  location = {sydney, Australia},
  doi = {10.1109/DSAA49011.2020.00097},
  url = {https://ieeexplore.ieee.org/document/9260072/},
  urldate = {2024-09-11},
  eventtitle = {2020 {{IEEE}} 7th {{International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  isbn = {978-1-72818-206-3},
  file = {/Users/alexander/Zotero/storage/DS27K4RZ/Orozco-Acosta et al. - 2020 - Bayesian Modeling Approach in Big Data Contexts an Application in Spatial Epidemiology.pdf}
}

@online{pengComprehensiveOverviewSurvey2020,
  title = {A {{Comprehensive Overview}} and {{Survey}} of {{Recent Advances}} in {{Meta-Learning}}},
  author = {Peng, Huimin},
  date = {2020-10-26},
  eprint = {2004.11149},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.11149},
  urldate = {2024-09-11},
  abstract = {This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexander/Zotero/storage/MCYFXZ3V/Peng - 2020 - A Comprehensive Overview and Survey of Recent Advances in Meta-Learning.pdf;/Users/alexander/Zotero/storage/TYKQMVTF/2004.html}
}

@article{saezSMOTEIPFAddressing2015,
  title = {{{SMOTE}}–{{IPF}}: {{Addressing}} the Noisy and Borderline Examples Problem in Imbalanced Classification by a Re-Sampling Method with Filtering},
  shorttitle = {{{SMOTE}}–{{IPF}}},
  author = {Sáez, José A. and Luengo, Julián and Stefanowski, Jerzy and Herrera, Francisco},
  date = {2015-01-10},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {291},
  pages = {184--203},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2014.08.051},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025514008561},
  urldate = {2024-09-11},
  abstract = {Classification datasets often have an unequal class distribution among their examples. This problem is known as imbalanced classification. The Synthetic Minority Over-sampling Technique (SMOTE) is one of the most well-know data pre-processing methods to cope with it and to balance the different number of examples of each class. However, as recent works claim, class imbalance is not a problem in itself and performance degradation is also associated with other factors related to the distribution of the data. One of these is the presence of noisy and borderline examples, the latter lying in the areas surrounding class boundaries. Certain intrinsic limitations of SMOTE can aggravate the problem produced by these types of examples and current generalizations of SMOTE are not correctly adapted to their treatment. This paper proposes the extension of SMOTE through a new element, an iterative ensemble-based noise filter called Iterative-Partitioning Filter (IPF), which can overcome the problems produced by noisy and borderline examples in imbalanced datasets. This extension results in SMOTE–IPF. The properties of this proposal are discussed in a comprehensive experimental study. It is compared against a basic SMOTE and its most well-known generalizations. The experiments are carried out both on a set of synthetic datasets with different levels of noise and shapes of borderline examples as well as real-world datasets. Furthermore, the impact of introducing additional different types and levels of noise into these real-world data is studied. The results show that the new proposal performs better than existing SMOTE generalizations for all these different scenarios. The analysis of these results also helps to identify the characteristics of IPF which differentiate it from other filtering approaches.},
  keywords = {Borderline examples,Imbalanced classification,Noise filters,Noisy data,SMOTE},
  file = {/Users/alexander/Zotero/storage/VFSACSP7/Sáez et al. - 2015 - SMOTE–IPF Addressing the noisy and borderline examples problem in imbalanced classification by a re.pdf;/Users/alexander/Zotero/storage/YMY6PGMI/S0020025514008561.html}
}

@inproceedings{sahniAidedSelectionSampling2021,
  title = {Aided {{Selection}} of {{Sampling Methods}} for {{Imbalanced Data Classification}}},
  author = {Sahni, Deep and Pappu, Satya Jayadev and Bhatt, Nirav},
  date = {2021-01-02},
  pages = {198--202},
  doi = {10.1145/3430984.3431029}
}

@thesis{schmidhuberEvolutionaryPrinciplesSelfreferential1987,
  title = {Evolutionary Principles in Self-Referential Learning, or on Learning How to Learn: {{The}} Meta-Meta-... Hook},
  shorttitle = {Evolutionary Principles in Self-Referential Learning, or on Learning How to Learn},
  author = {Schmidhuber, Jürgen},
  date = {1987},
  institution = {Technische Universität München},
  url = {https://mediatum.ub.tum.de/813180},
  urldate = {2024-09-10},
  file = {/Users/alexander/Zotero/storage/PZIF3J8N/813180.html}
}

@inproceedings{seiffertRUSBoostImprovingClassification2008,
  title = {{{RUSBoost}}: {{Improving}} Classification Performance When Training Data Is Skewed},
  shorttitle = {{{RUSBoost}}},
  booktitle = {2008 19th {{International Conference}} on {{Pattern Recognition}}},
  author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and Van Hulse, Jason and Napolitano, Amri},
  date = {2008-12},
  pages = {1--4},
  publisher = {IEEE},
  location = {Tampa, FL, USA},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2008.4761297},
  url = {http://ieeexplore.ieee.org/document/4761297/},
  urldate = {2024-09-11},
  eventtitle = {2008 19th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-4244-2174-9}
}

@article{shahapureClusterQualityAnalysis2020,
  title = {Cluster {{Quality Analysis Using Silhouette Score}}},
  author = {Shahapure, Ketan Rajshekhar and Nicholas, Charles},
  date = {2020-10},
  journaltitle = {2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)},
  pages = {747--748},
  publisher = {IEEE},
  location = {sydney, Australia},
  doi = {10.1109/DSAA49011.2020.00096},
  url = {https://ieeexplore.ieee.org/document/9260048/},
  urldate = {2024-09-11},
  abstract = {Clustering is an important phase in data mining. Selecting the number of clusters in a clustering algorithm, e.g. choosing the best value of k in the various k-means algorithms [1], can be difficult. We studied the use of silhouette scores and scatter plots to suggest, and then validate, the number of clusters we specified in running the k-means clustering algorithm on two publicly available data sets. Scikit-learn's [4] silhouette score method, which is a measure of the quality of a cluster, was used to find the mean silhouette co-efficient of all the samples for different number of clusters. The highest silhouette score indicates the optimal number of clusters. We present several instances of utilizing the silhouette score to determine the best value of k for those data sets.},
  eventtitle = {2020 {{IEEE}} 7th {{International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  isbn = {9781728182063},
  file = {/Users/alexander/Zotero/storage/NUEAZJRJ/Shahapure and Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf}
}

@article{singhWeightedKnearestNeighbor2020,
  title = {Weighted K-Nearest Neighbor Based Data Complexity Metrics for Imbalanced Datasets},
  author = {Singh, Deepika and Gosain, Anjana and Saha, Anju},
  date = {2020},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {13},
  number = {4},
  pages = {394--404},
  issn = {1932-1872},
  doi = {10.1002/sam.11463},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11463},
  urldate = {2024-09-11},
  abstract = {Empirical behavior of a classifier depends strongly on the characteristics of the underlying imbalanced dataset; therefore, an analysis of intrinsic data complexity would appear to be vital in order to choose classifiers suitable for particular problems. Data complexity metrics (CMs), a fairly recent proposal, identify dataset features which imply some difficulty for the classification task and identify relationships with classifier accuracy. In this paper, we introduce two CMs for imbalanced datasets, which help in explaining the factors responsible for the deterioration in classifier performance. These metrics are based on the weighted k-nearest neighbors approach. The experiments are performed in MATLAB software using 48 simulated datasets and 22 real-world datasets for different choices of neighborhood size k considered as 3, 5, 7, 9, 11. The results help to illustrate the usefulness of the proposed metrics.},
  langid = {english},
  keywords = {Bayes error,class imbalance,classification,complexity metrics,data complexity,data-level algorithms,imbalance ratio,overlapping,small disjuncts,undersampled},
  file = {/Users/alexander/Zotero/storage/XAHF58RR/Singh et al. - 2020 - Weighted k-nearest neighbor based data complexity metrics for imbalanced datasets.pdf;/Users/alexander/Zotero/storage/78LDKD7F/sam.html}
}

@article{smithInstanceLevelAnalysis2014,
  title = {An Instance Level Analysis of Data Complexity},
  author = {Smith, Michael R. and Martinez, Tony and Giraud-Carrier, Christophe},
  date = {2014-05},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {95},
  number = {2},
  pages = {225--256},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-013-5422-z},
  url = {http://link.springer.com/10.1007/s10994-013-5422-z},
  urldate = {2024-09-11},
  langid = {english},
  file = {/Users/alexander/Zotero/storage/I5HEHB5B/Smith et al. - 2014 - An instance level analysis of data complexity.pdf}
}

@inproceedings{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
  urldate = {2024-09-11},
  abstract = {We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  file = {/Users/alexander/Zotero/storage/KUYWDR2L/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf}
}

@article{steinleyPropertiesHubertArabieAdjusted2004,
  title = {Properties of the {{Hubert-Arabie}} Adjusted {{Rand}} Index},
  author = {Steinley, Douglas},
  date = {2004-09},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychol Methods},
  volume = {9},
  number = {3},
  eprint = {15355155},
  eprinttype = {pmid},
  pages = {386--396},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.9.3.386},
  abstract = {This article provides an investigation of cluster validation indices that relates 4 of the indices to the L. Hubert and P. Arabie (1985) adjusted Rand index--the cluster validation measure of choice (G. W. Milligan \& M. C. Cooper, 1986). It is shown how these other indices can be "roughly" transformed into the same scale as the adjusted Rand index. Furthermore, in-depth explanations are given of why classification rates should not be used in cluster validation research. The article concludes by summarizing several properties of the adjusted Rand index across many conditions and provides a method for testing the significance of observed adjusted Rand indices.},
  langid = {english},
  keywords = {Artificial Intelligence,Cluster Analysis,Humans,Mathematical Computing,Monte Carlo Method,Neural Networks Computer,Reproducibility of Results}
}

@article{steinleyPropertiesHubertArableAdjusted2004,
  title = {Properties of the {{Hubert-Arable Adjusted Rand Index}}.},
  author = {Steinley, Douglas},
  date = {2004},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {9},
  number = {3},
  pages = {386--396},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.3.386},
  url = {https://doi.apa.org/doi/10.1037/1082-989X.9.3.386},
  urldate = {2024-09-11},
  langid = {english}
}

@article{tangIoMTbasedGeriatricCare2019,
  title = {An {{IoMT-based}} Geriatric Care Management System for Achieving Smart Health in Nursing Homes},
  author = {Tang, Valerie and Choy, K.L. and Ho, G.T.S. and Lam, H.Y. and Tsang, Y.P.},
  date = {2019-09-09},
  journaltitle = {Industrial Management \& Data Systems},
  shortjournal = {IMDS},
  volume = {119},
  number = {8},
  pages = {1819--1840},
  issn = {0263-5577},
  doi = {10.1108/IMDS-01-2019-0024},
  url = {https://www.emerald.com/insight/content/doi/10.1108/IMDS-01-2019-0024/full/html},
  urldate = {2024-09-11},
  abstract = {Purpose               The purpose of this paper is to develop an Internet of medical things (IoMT)-based geriatric care management system (I-GCMS), integrating IoMT and case-based reasoning (CBR) in order to deal with the global concerns of the increasing demand for elderly care service in nursing homes.                                         Design/methodology/approach               The I-GCMS is developed under the IoMT environment to collect real-time biometric data for total health monitoring. When the health of an elderly deteriorates, the CBR is used to revise and generate the customized care plan, and hence support and improve the geriatric care management (GCM) service in nursing homes.                                         Findings               A case study is conducted in a nursing home in Taiwan to evaluate the performance of the I-GCMS. Under the IoMT environment, the time saving in executing total health monitoring helps improve the daily operation effectiveness and efficiency. In addition, the proposed system helps leverage a proactive approach in modifying the content of a care plan in response to the change of health status of elderly.                                         Originality/value               Considering the needs for demanding and accurate healthcare services, this is the first time that IoMT and CBR technologies have been integrated in the field of GCM. This paper illustrates how to seamlessly connect various sensors to capture real-time biometric data to the I-GCMS platform for responsively supporting decision making in the care plan modification processes. With the aid of I-GCMS, the efficiency in executing the daily routine processes and the quality of healthcare services can be improved.},
  langid = {english}
}

@article{tomekExperimentEditedNearestNeighbor1976,
  title = {An {{Experiment}} with the {{Edited Nearest-Neighbor Rule}}},
  author = {Tomek, Ivan},
  date = {1976-06},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  shortjournal = {IEEE Trans. Syst., Man, Cybern.},
  volume = {SMC-6},
  number = {6},
  pages = {448--452},
  issn = {0018-9472, 2168-2909},
  doi = {10.1109/TSMC.1976.4309523},
  url = {http://ieeexplore.ieee.org/document/4309523/},
  urldate = {2024-09-11}
}

@article{tomekTwoModificationsCNN1976,
  title = {Two {{Modifications}} of {{CNN}}},
  author = {Tomek, Ivan},
  date = {1976-11},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  shortjournal = {IEEE Trans. Syst., Man, Cybern.},
  volume = {SMC-6},
  number = {11},
  pages = {769--772},
  issn = {0018-9472, 2168-2909},
  doi = {10.1109/TSMC.1976.4309452},
  url = {http://ieeexplore.ieee.org/document/4309452/},
  urldate = {2024-09-11}
}

@inproceedings{tVLSIImplementationReed2020,
  title = {{{VLSI Implementation}} of {{Reed Solomon Codes}}},
  booktitle = {2020 {{Fourth International Conference}} on {{Computing Methodologies}} and {{Communication}} ({{ICCMC}})},
  author = {T, Syam Krishnan and Chalil, Anu and Kn, Sreehari},
  date = {2020-03},
  pages = {280--284},
  publisher = {IEEE},
  location = {Erode, India},
  doi = {10.1109/ICCMC48092.2020.ICCMC-00052},
  url = {https://ieeexplore.ieee.org/document/9076498/},
  urldate = {2024-09-11},
  eventtitle = {2020 {{Fourth International Conference}} on {{Computing Methodologies}} and {{Communication}} ({{ICCMC}})},
  isbn = {978-1-72814-889-2}
}

@online{vanschorenMetaLearningSurvey2018,
  title = {Meta-{{Learning}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}}},
  author = {Vanschoren, Joaquin},
  date = {2018},
  doi = {10.48550/ARXIV.1810.03548},
  url = {https://arxiv.org/abs/1810.03548},
  urldate = {2024-09-11},
  abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
  pubstate = {prepublished},
  version = {1},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/alexander/Zotero/storage/9PNEPY7W/Vanschoren - 2018 - Meta-Learning A Survey.pdf}
}

@article{vinhInformationTheoreticMeasures2010,
  title = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}: {{Variants}}, {{Properties}}, {{Normalization}} and {{Correction}} for {{Chance}}},
  shorttitle = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}},
  author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
  date = {2010},
  journaltitle = {Journal of Machine Learning Research},
  volume = {11},
  number = {95},
  pages = {2837--2854},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v11/vinh10a.html},
  urldate = {2024-09-11},
  abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0,1] range better than other normalized variants.},
  file = {/Users/alexander/Zotero/storage/L2MJHZMC/Vinh et al. - 2010 - Information Theoretic Measures for Clusterings Comparison Variants, Properties, Normalization and C.pdf}
}

@online{vinyalsMatchingNetworksOne2017,
  title = {Matching {{Networks}} for {{One Shot Learning}}},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
  date = {2017-12-29},
  eprint = {1606.04080},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.04080},
  urldate = {2024-09-11},
  abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexander/Zotero/storage/IZGYYX52/Vinyals et al. - 2017 - Matching Networks for One Shot Learning.pdf;/Users/alexander/Zotero/storage/QLT8IQRR/1606.html}
}

@article{waltDataMeasuresThat2008,
  title = {Data Measures That Characterise Classification Problems},
  author = {family=Walt, given=Van, prefix=der, useprefix=false and Maarten, Christiaan},
  date = {2008-09-09},
  publisher = {University of Pretoria},
  url = {https://repository.up.ac.za/handle/2263/27624},
  urldate = {2024-09-11},
  abstract = {We have a wide-range of classifiers today that are employed in numerous applications, from credit scoring to speech-processing, with great technical and commercial success. No classifier, however, exists that will outperform all other classifiers on all classification tasks, and the process of classifier selection is still mainly one of trial and error. The optimal classifier for a classification task is determined by the characteristics of the data set employed; understanding the relationship between data characteristics and the performance of classifiers is therefore crucial to the process of classifier selection. Empirical and theoretical approaches have been employed in the literature to define this relationship. None of these approaches have, however, been very successful in accurately predicting or explaining classifier performance on real-world data. We use theoretical properties of classifiers to identify data characteristics that influence classifier performance; these data properties guide us in the development of measures that describe the relationship between data characteristics and classifier performance. We employ these data measures on real-world and artificial data to construct a meta-classification system. We use theoretical properties of classifiers to identify data characteristics that influence classifier performance; these data properties guide us in the development of measures that describe the relationship between data characteristics and classifier performance. We employ these data measures on real-world and artificial data to construct a meta-classification system. The purpose of this meta-classifier is two-fold: (1) to predict the classification performance of real-world classification tasks, and (2) to explain these predictions in order to gain insight into the properties of real-world data. We show that these data measures can be employed successfully to predict the classification performance of real-world data sets; these predictions are accurate in some instances but there is still unpredictable behaviour in other instances. We illustrate that these data measures can give valuable insight into the properties and data structures of real-world data; these insights are extremely valuable for high-dimensional classification problems.},
  annotation = {Accepted: 2013-09-07T11:52:19Z},
  file = {/Users/alexander/Zotero/storage/R6GGY2RH/Walt and Maarten - 2008 - Data measures that characterise classification problems.pdf}
}

@online{wangGeneralizingFewExamples2020,
  title = {Generalizing from a {{Few Examples}}: {{A Survey}} on {{Few-Shot Learning}}},
  shorttitle = {Generalizing from a {{Few Examples}}},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M.},
  date = {2020-03-29},
  eprint = {1904.05046},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.05046},
  url = {http://arxiv.org/abs/1904.05046},
  urldate = {2024-09-11},
  abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexander/Zotero/storage/BRZTE6Z8/Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-Shot Learning.pdf;/Users/alexander/Zotero/storage/U5G28QYB/1904.html}
}

@incollection{wangImprovedMethodsClassification2015,
  title = {Improved {{Methods}} for {{Classification}}, {{Prediction}}, and {{Design}} of {{Antimicrobial Peptides}}},
  booktitle = {Computational {{Peptidology}}},
  author = {Wang, Guangshun},
  editor = {Zhou, Peng and Huang, Jian},
  date = {2015},
  pages = {43--66},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4939-2285-7_3},
  url = {https://doi.org/10.1007/978-1-4939-2285-7_3},
  urldate = {2024-09-10},
  abstract = {Peptides with diverse amino acid sequences, structures, and functions are essential players in biological systems. The construction of well-annotated databases not only facilitates effective information management, search, and mining but also lays the foundation for developing and testing new peptide algorithms and machines. The antimicrobial peptide database (APD) is an original construction in terms of both database design and peptide entries. The host defense antimicrobial peptides (AMPs) registered in the APD cover the five kingdoms (bacteria, protists, fungi, plants, and animals) or three domains of life (bacteria, archaea, and eukaryota). This comprehensive database (http://aps.unmc.edu/AP) provides useful information on peptide discovery timeline, nomenclature, classification, glossary, calculation tools, and statistics. The APD enables effective search, prediction, and design of peptides with antibacterial, antiviral, antifungal, antiparasitic, insecticidal, spermicidal, anticancer activities, chemotactic, immune modulation, or antioxidative properties. A universal classification scheme is proposed herein to unify innate immunity peptides from a variety of biological sources. As an improvement, the upgraded APD makes predictions based on the database-defined parameter space and provides a list of the sequences most similar to natural AMPs. In addition, the powerful pipeline design of the database search engine laid a solid basis for designing novel antimicrobials to combat resistant superbugs, viruses, fungi, or parasites. This comprehensive AMP database is a useful tool for both research and education.},
  isbn = {978-1-4939-2285-7},
  langid = {english},
  keywords = {Ab initio design,Database filtering tech,Database screen,Peptide design,Peptide prediction,Universal peptide classification},
  file = {/Users/alexander/Zotero/storage/YVKLZ9MD/Wang - 2015 - Improved Methods for Classification, Prediction, and Design of Antimicrobial Peptides.pdf}
}

@inproceedings{wardhaniCrossvalidationMetricsEvaluating2019,
  title = {Cross-Validation {{Metrics}} for {{Evaluating Classification Performance}} on {{Imbalanced Data}}},
  booktitle = {2019 {{International Conference}} on {{Computer}}, {{Control}}, {{Informatics}} and Its {{Applications}} ({{IC3INA}})},
  author = {Wardhani, Ni Wayan Surya and Rochayani, Masithoh Yessi and Iriany, Atiek and Sulistyono, Agus Dwi and Lestantyo, Prayudi},
  date = {2019-10},
  pages = {14--18},
  publisher = {IEEE},
  location = {Tangerang, Indonesia},
  doi = {10.1109/IC3INA48034.2019.8949568},
  url = {https://ieeexplore.ieee.org/document/8949568/},
  urldate = {2024-09-11},
  eventtitle = {2019 {{International Conference}} on {{Computer}}, {{Control}}, {{Informatics}} and Its {{Applications}} ({{IC3INA}})},
  isbn = {978-1-72815-540-1}
}

@article{wilsonAsymptoticPropertiesNearest1972,
  title = {Asymptotic {{Properties}} of {{Nearest Neighbor Rules Using Edited Data}}},
  author = {Wilson, Dennis L.},
  date = {1972-07},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  shortjournal = {IEEE Trans. Syst., Man, Cybern.},
  volume = {SMC-2},
  number = {3},
  pages = {408--421},
  issn = {0018-9472, 2168-2909},
  doi = {10.1109/TSMC.1972.4309137},
  url = {http://ieeexplore.ieee.org/document/4309137/},
  urldate = {2024-09-11}
}

@article{xieValidityMeasureFuzzy1991,
  title = {A Validity Measure for Fuzzy Clustering},
  author = {Xie, X.L. and Beni, G.},
  year = {Aug./1991},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
  volume = {13},
  number = {8},
  pages = {841--847},
  issn = {01628828},
  doi = {10.1109/34.85677},
  url = {http://ieeexplore.ieee.org/document/85677/},
  urldate = {2024-09-11}
}

@inproceedings{zakrzewskaDynamicAlgorithmLocal2015,
  title = {A {{Dynamic Algorithm}} for {{Local Community Detection}} in {{Graphs}}},
  booktitle = {Proceedings of the 2015 {{IEEE}}/{{ACM International Conference}} on {{Advances}} in {{Social Networks Analysis}} and {{Mining}} 2015},
  author = {Zakrzewska, Anita and Bader, David A.},
  date = {2015-08-25},
  pages = {559--564},
  publisher = {ACM},
  location = {Paris France},
  doi = {10.1145/2808797.2809375},
  url = {https://dl.acm.org/doi/10.1145/2808797.2809375},
  urldate = {2024-09-11},
  eventtitle = {{{ASONAM}} '15: {{Advances}} in {{Social Networks Analysis}} and {{Mining}} 2015},
  isbn = {978-1-4503-3854-7},
  langid = {english}
}

@article{zhangClassImbalanceAwareMultiLabel2022,
  title = {Towards {{Class-Imbalance Aware Multi-Label Learning}}},
  author = {Zhang, Min-Ling and Li, Yu-Kun and Yang, Hao and Liu, Xu-Ying},
  date = {2022-06},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {52},
  number = {6},
  pages = {4459--4471},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.3027509},
  url = {https://ieeexplore.ieee.org/document/9262911/?arnumber=9262911},
  urldate = {2024-09-11},
  abstract = {Multi-label learning deals with training examples each represented by a single instance while associated with multiple class labels. Due to the exponential number of possible label sets to be considered by the predictive model, it is commonly assumed that label correlations should be well exploited to design an effective multi-label learning approach. On the other hand, class-imbalance stands as an intrinsic property of multi-label data which significantly affects the generalization performance of the multi-label predictive model. For each class label, the number of training examples with positive labeling assignment is generally much less than those with negative labeling assignment. To deal with the class-imbalance issue for multi-label learning, a simple yet effective class-imbalance aware learning strategy called cross-coupling aggregation (COCOA) is proposed in this article. Specifically, COCOA works by leveraging the exploitation of label correlations as well as the exploration of class-imbalance simultaneously. For each class label, a number of multiclass imbalance learners are induced by randomly coupling with other labels, whose predictions on the unseen instance are aggregated to determine the corresponding labeling relevancy. Extensive experiments on 18 benchmark datasets clearly validate the effectiveness of COCOA against state-of-the-art multi-label learning approaches especially in terms of imbalance-specific evaluation metrics.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Class-imbalance,Correlation,Couplings,cross-coupling aggregation (COCOA),Labeling,machine learning,multi-label learning,Predictive models,Task analysis,Technological innovation,Training},
  file = {/Users/alexander/Zotero/storage/J3QRD89K/Zhang et al. - 2022 - Towards Class-Imbalance Aware Multi-Label Learning.pdf;/Users/alexander/Zotero/storage/FYFMNMTT/9262911.html}
}

@article{zhaoAptamerModifiedMonolithicCapillary2008,
  title = {Aptamer-{{Modified Monolithic Capillary Chromatography}} for {{Protein Separation}} and {{Detection}}},
  author = {Zhao, Qiang and Li, Xing-Fang and Le, X. Chris},
  date = {2008-05-01},
  journaltitle = {Analytical Chemistry},
  shortjournal = {Anal. Chem.},
  volume = {80},
  number = {10},
  pages = {3915--3920},
  publisher = {American Chemical Society},
  issn = {0003-2700},
  doi = {10.1021/ac702567x},
  url = {https://doi.org/10.1021/ac702567x},
  urldate = {2024-09-10},
  abstract = {A capillary chromatography technique was developed for the separation and detection of proteins, taking advantage of the specific affinity of aptamers and the porous property of the monolith. A biotinylated DNA aptamer targeting cytochrome c was successfully immobilized on a streptavidin-modified polymer monolithic capillary column. The aptamer, having a G-quartet structure, could bind to both cytochrome c and thrombin, enabling the separation of these proteins from each other and from the unretained proteins. Elution of strongly bound proteins was achieved by increasing the ionic strength of the mobile phase. The following proteins were tested using the aptamer affinity monolithic columns: human immunoglobulin G (IgG), hemoglobin, transferrin, human serum albumin, cytochrome c, and thrombin. Determination of cytochrome c and thrombin spiked into dilute serum samples showed no interference from the serum matrix. The benefit of porous properties of the affinity monolithic column was demonstrated by selective capture and preconcentration of thrombin at low ionic strength and subsequent rapid elution at high ionic strength. The combination of the polymer monolithic column and the aptamer affinities makes the aptamer-modified monolithic columns useful for protein detection and separation.},
  file = {/Users/alexander/Zotero/storage/UZ5UGWGE/Zhao et al. - 2008 - Aptamer-Modified Monolithic Capillary Chromatography for Protein Separation and Detection.pdf}
}

@article{zhaoProteinClassificationImbalanced2008,
  title = {Protein Classification with Imbalanced Data},
  author = {Zhao, Xing‐Ming and Li, Xin and Chen, Luonan and Aihara, Kazuyuki},
  date = {2008-03},
  journaltitle = {Proteins: Structure, Function, and Bioinformatics},
  shortjournal = {Proteins},
  volume = {70},
  number = {4},
  pages = {1125--1132},
  issn = {0887-3585, 1097-0134},
  doi = {10.1002/prot.21870},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/prot.21870},
  urldate = {2024-09-11},
  abstract = {Abstract             Generally, protein classification is a multi‐class classification problem and can be reduced to a set of binary classification problems, where one classifier is designed for each class. The proteins in one class are seen as positive examples while those outside the class are seen as negative examples. However, the imbalanced problem will arise in this case because the number of proteins in one class is usually much smaller than that of the proteins outside the class. As a result, the imbalanced data cause classifiers to tend to overfit and to perform poorly in particular on the minority class.                            This article presents a new technique for protein classification with imbalanced data. First, we propose a new algorithm to overcome the imbalanced problem in protein classification with a new sampling technique and a committee of classifiers. Then, classifiers trained in different feature spaces are combined together to further improve the accuracy of protein classification. The numerical experiments on benchmark datasets show promising results, which confirms the effectiveness of the proposed method in terms of accuracy. The Matlab code and supplementary materials are available at               http://eserver2.sat.iis.u‐tokyo.ac.jp/∼xmzhao/proteins.html               . Proteins 2008. © 2007 Wiley‐Liss, Inc.},
  langid = {english}
}

@inproceedings{zhouTaskSimilarityAware2021,
  title = {Task Similarity Aware Meta Learning: Theory-Inspired Improvement on {{MAML}}},
  shorttitle = {Task Similarity Aware Meta Learning},
  author = {Zhou, Pan and Zou, Yingtian and Yuan, Xiaotong and Feng, Jiashi and Xiong, Caiming and Hoi, S.},
  date = {2021},
  url = {https://www.semanticscholar.org/paper/Task-similarity-aware-meta-learning%3A-improvement-on-Zhou-Zou/79dfbde81b4c4622ad44c4223f5e27af99847fab},
  urldate = {2024-09-11},
  abstract = {Few-shot learning ability is heavily desired for machine intelligence. By meta-learning a model initialization from training tasks with fast adaptation ability to new tasks, model-agnostic meta-learning (MAML) has achieved remarkable success in a number of few-shot learning applications. However, theoretical understandings on the learning ability of MAML remain absent yet, hindering developing new and more advanced meta learning methods in a principle way. In this work, we solve this problem by theoretically justifying the fast adaptation capability of MAML when applied to new tasks. Specifically, we prove that the learnt meta-initialization can quickly adapt to new tasks with only a few steps of gradient descent. This result, for the first time, explicitly reveals the benefits of the unique designs in MAML. Then we propose a theory-inspired task similarity aware MAML which clusters tasks into multiple groups according to the estimated optimal model parameters and learns group-specific initializations. The proposed method improves upon MAML by speeding up the adaptation and giving stronger few-shot learning ability. Experimental results on the few-shot classification tasks testify its advantages.},
  eventtitle = {Conference on {{Uncertainty}} in {{Artificial Intelligence}}}
}
